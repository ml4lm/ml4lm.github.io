<!DOCTYPE html>
<!-- saved from url=(0045)https://mbzuai-cl.github.io/2023/programday2/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!--<base href=".">--><base href=".">
    <link rel="shortcut icon" type="image/png" href="https://mbzuai-cl.github.io/2023/assets/favicon.png">
    <link rel="stylesheet" type="text/css" media="all" href="./MLLM2024_day2_files/main.css">
    <meta name="description" content="MBZUAI Machine Learning for Large Models Workshop 2024">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>1st MBZUAI Machine Learning for Large Models Workshop 2024</title>
</head>

<body>

    <div class="banner">
        <img src="./MLLM2024_day2_files/banner.jpeg" alt="MBZUAI Machine Learning for Large Models 2024 Banner">
        <div class="top-left">
           <span class="title2">1st MBZUAI Workshop on</span>
            <br><br> <span class="title1">Machine Learning for Large Models</span> 
            <!-- <br><br>
            <span class="year">Empowering Sustainable Futures</span> -->
        </div>
        <div class="bottom-right">
            June 3-4, 2024 <br> MBZUAI, Abu Dhabi
        </div>
    </div>

   <table class="navigation">
        <tbody><tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href="index.html">Home</a>
            </td>
            <td class="navigation">
                <a title="Speakers List" href="speakerlist.html">Speakers List</a> 
            </td>
            <td class="navigation">
                <a title="Conference Program" href="day1.html">Program Day 1</a> 
            </td>
            <!-- <td class="navigation">
                <a title="Conference Program" href="https://mbzuai.ac.ae/">MBZUAI</a> 
            </td> -->
            <td class="navigation">
                <a title="Conference Program Day 2" href="day2.html">Program Day 2</a> 
            </td>
        </tr>
    </tbody></table>

    <h2>Day 2 Program (June 4, Tue)</h2>
     <table id="Timothy Baldwin">
        <tbody><tr>
            <td class="date" rowspan="3">
                9:00am
            </td>
            <td class="title">
                Safer, more relible, more diverse LLMs
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://eltimster.github.io/www/"><b>Timothy Baldwin</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                The recent surge in generative large language models (LLMs) has
                created even greater challenges for NLP evaluation. In this talk, I
                will cover a range of LLM evaluation initiatives covering issues
                including: multilingual and multicultural capabilities of LLMs; the
                ability of models to capture different aspects of negation;
                uncertainty quantification; and model safety.
        </td></tr>
    </tbody></table>


    <table id="Gus Xia">
        <tbody><tr>
            <td class="date" rowspan="3">
             9:30am
            </td>
            <td class="title">
                A Musical View on Multimodal Large Language Models: Hierarchical Modeling, Control, and AI Partnerships
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/"><b>Gus Xia</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                In this presentation, Gus will delve into three pioneering studies focused on developing multimodal language models for music AI. Firstly, he will discuss whole-song generation through cascaded diffusion modeling, which integrates hierarchical music language structures into diffusion models for enhanced sample-efficient learning. Secondly, he will introduce "coco-mulla," the inaugural "controlnet" fashion model that adeptly applies content-based controls to large-scale audio generation tasks. Lastly, Gus will present Flute X GPT, which introduces LAUI (LLM-Agent User Interface) as a potential next-generation Human-Computer Interaction (HCI) paradigm after Graphical User Interface (GUI). This system capitalizes on a nuanced understanding of both users and tutorial software to innovate customized music-learning experiences.
        </td></tr>
    </tbody></table>


    <table id="Michalis Vazirgiannis">
        <tbody><tr>
            <td class="date" rowspan="3">
                10:00am
            </td>
            <td class="title">
                Multimodal Generative AI and applications to biomedical domain
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://scholar.google.gr/citations?user=aWGJYcMAAAAJ&hl=en"><b>Michalis Vazirgiannis</b></a> (Ecole Polytechnique/MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Graph generative models are recently gaining significant interest in current application domains. They are commonly used to model social networks, knowledge graphs, molecules and proteins. In this talk we will present the potential of graph generative models and recent relevant efforts in the biomedical domain. More specifically we present a novel architecture that generates medical records as graphs with privacy guarantees. We capitalize and modify the graph Variational autoencoders (VAEs) architecture.   Finally we present ongoing work and research directions for multi modal generative models involving graphs and applications to molecule generation with LLMs and graphs.
        </td></tr>
    </tbody></table>
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                10:30am
            </td>
            <td class="title-special">
                Coffee Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>


    <table id="Ahmed Salem">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:00am
            </td>
            <td class="title">
                The Crescendo Effect: Understanding Multi-Turn Jailbreaks in Large Language Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://ahmedsalem2.github.io/"><b>Ahmed Salem</b></a> (Microsoft)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Large Language Models (LLMs) are powerful tools capable of generating content that might violate the principles of Responsible AI (RAI). These 
                models are engineered to steer clear of engaging in such illegal or unethical topics through a process known as alignment. This process 
                typically focuses on preventing single-turn jailbreaks, which are deliberate attacks attempting to circumvent the safety alignment. In this 
                talk, I will introduce a new category of jailbreaks that are executed over multiple turns, and I will showcase a specific multi-turn 
                jailbreak called Crescendo. Crescendo interacts with the model in a seemingly benign manner and gradually leads to a successful jailbreak. 
                The talk will also cover Crescendomation, an automated tool designed to execute the Crescendo attack. Finally, I will discuss the complexities 
                involved in evaluating jailbreaks and share the results of Crescendomation's assessment and the lessons learned.
        </td></tr>
    </tbody></table>

    <table id="Guokan Shang">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:30am
            </td>
            <td class="title">
                Evaluating Linguistic Diversity of Large Language Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://shang.tech/aboutme/"><b>Guokan Shang</b></a> (MBZUAI France Lab)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Recently, Large Language Models (LLMs) have gained widespread recognition and usage. However, their 
                evaluation predominantly concentrates on task-solving performance. Diverging from this usual emphasis, 
                we focus on linguistic perspectives, specifically linguistic diversity, a fundamentally important but 
                significantly overlooked aspect of language generation. The talk begins with a taxonomy of linguistic 
                diversity evaluation metrics, followed by a presentation of our recent study: "The Curious Decline of 
                Linguistic Diversity: Training Language Models on Synthetic Text". Our research found that training LLMs 
                on predecessor-generated text—where language models are trained on synthetic data produced by previous 
                models—causes a consistent decrease in the lexical, syntactic, and semantic diversity of the model outputs 
                through successive iterations. This decline is particularly notable for tasks demanding high levels of 
                creativity. Our study highlights the need for careful consideration of the long-term effects of such 
                training approaches, particularly concerning the preservation by LLMs of human linguistic richness.
        </td></tr>
    </tbody></table>
 
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                12:00pm
            </td>
            <td class="title-special">
                Lunch
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>
    <table id="Preslav Nakov">
        <tbody><tr>
            <td class="date" rowspan="3">
                2:00pm
            </td>
            <td class="title">
                Factuality Challenges in the Era of Large Language Models: Can We Keep LLMs Safe and Factual?
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov/"><b>Preslav Nakov</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We will discuss the risks, the challenges, and the opportunities that Large Language Models (LLMs) bring regarding factuality. We will then present a number of LLM fact-checking tools recently developed at MBZUAI: (i) Factcheck-Bench, a fine-grained evaluation benchmark and framework for fact-checking the output of LLMs, (ii) OpenFactVerification (Loki), an open-source tool for fact-checking the output of LLMs, developed based on Factcheck-Bench and optimized for speed and quality, (iii) OpenFactCheck, a framework for building customized fact-checking systems and for benchmarking entire LLMs, and (iv) LM-Polygraph, a tool to predict an LLM's uncertainty in its output using cheap and fast uncertainty quantification techniques. Finally, we will discuss the safety mechanisms we incorporated in Jais-chat, the world's best open Arabic-centric foundation and instruction-tuned LLM.
        </td></tr>
    </tbody></table>
    <table id="Karthik Nandakumar">
        <tbody><tr>
            <td class="date" rowspan="3">
                2:30pm
            </td>
            <td class="title">
                Safety and Robustness of Large Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.sprintai.org/nkarthik/"><b>Karthik Nandakumar</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                While large machine learning models are a valuable tool for solving tough problems in many domains including text, speech, and vision, state-of-the-art large models are vulnerable to numerous security and privacy threats. In the first part of the talk, we briefly review these threats and identify key unsolved challenges. In particular, we will focus on adversarial attacks and defense mechanisms. For large generative models, alignment with human values is even more critical to mitigate the risk of unintended consequences. At the same time, care must be taken to ensure that prevalent human biases do not creep into large models. These challenges can be addressed by following well-known cybersecurity practices such as red and blue teaming, with the red team attempting to expose the vulnerabilities of the large models and the blue team devoted to plugging these loopholes. Finally, we focus on the challenge of preserving the privacy of data used in machine learning. We consider the scenario where data generated by multiple organizations or individuals needs to be leveraged to develop large models. While federated learning is typically used in such scenarios, this framework is highly inefficient for large models, necessitating more efficient learning algorithms and collaboration protocols.
        </td></tr>
    </tbody></table>
    
   
     <table id="Nikhil Kandpal">
        <tbody><tr>
            <td class="date" rowspan="3">
                3:00pm
            </td>
            <td class="title">
                Understanding Language Models Through the Lens of their Training Data
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://nkandpa2.github.io/"><b>Nikhil Kandpal</b></a> (University of Toronto)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                The behaviors of language models are non-trivially influenced by their training data, presenting a significant challenge in understanding how properties of training datasets drive model responses. This talk explores methodologies to quantify these influences despite complexities introduced by the black-box nature of language models and large-scale training datasets. I will discuss key findings from recent studies that link high-level model behaviors, such as memorization and fact acquisition, to global characteristics of their training data. Additionally, I will discuss recent advances in techniques like semi-parametric language modeling and retrieval-augmented generation, which offer new ways to trace model outputs back to individual training instances. 
        </td></tr>
    </tbody></table>
    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                3:30pm
            </td>
            <td class="title-special">
                Coffee Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>

    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                4:00pm-5:00pm
            </td>
            <td class="title-special">
                Panel Discussion
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>


    <!-- <table id="Danqi Chen">
        <tbody><tr>
            <td class="date" rowspan="3">
                9:30am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.cs.princeton.edu/~danqic/"><b>Danqi Chen</b></a> (Princeton University)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    <table id="Samuel Horvath">
        <tbody><tr>
            <td class="date" rowspan="3">
             9:50am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://samuelhorvath.github.io/"><b>Samuel Horvath</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    <table id="Salman Khan">
        <tbody><tr>
            <td class="date" rowspan="3">
                10:10am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://salman-h-khan.github.io/"><b>Salman Khan</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>



    <table id="Preslav Nakov">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:00am
            </td>
            <td class="title">
                Factuality Challenges in the Era of Large Language Models: Can We Keep LLMs Safe and Factual?
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov/"><b>Preslav Nakov</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We will discuss the risks, the challenges, and the opportunities that Large Language Models (LLMs) bring regarding factuality. We will then present a number of LLM fact-checking tools recently developed at MBZUAI: (i) Factcheck-Bench, a fine-grained evaluation benchmark and framework for fact-checking the output of LLMs, (ii) OpenFactVerification (Loki), an open-source tool for fact-checking the output of LLMs, developed based on Factcheck-Bench and optimized for speed and quality, (iii) OpenFactCheck, a framework for building customized fact-checking systems and for benchmarking entire LLMs, and (iv) LM-Polygraph, a tool to predict an LLM's uncertainty in its output using cheap and fast uncertainty quantification techniques. Finally, we will discuss the safety mechanisms we incorporated in Jais-chat, the world's best open Arabic-centric foundation and instruction-tuned LLM.
        </td></tr>
    </tbody></table>

    <table id="Karthik Nandakumar">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:20am
            </td>
            <td class="title">
                To be determined.
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.sprintai.org/nkarthik/"><b>Karthik Nandakumar</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                To be determined.
        </td></tr>
    </tbody></table>

    
    

    <table id="Maxim Panov">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:40am
            </td>
            <td class="title">
                Uncertainty Quantification for Generative Language Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/maxim-panov/"><b>Maxim Panov</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                The widespread deployment of large language models (LLMs) has made ML-based applications even more vulnerable to risks of causing various forms of harm to users. For example, models often “hallucinate”, i.e., fabricate facts without providing users an apparent means to validate their statements. Uncertainty estimation (UE) methods could be used to detect unreliable generations unlocking the safer and more responsible use of LLMs in practice. UE methods for generative LLMs are a subject of cutting-edge research, which is currently quite scarce and scattered. We systemize these efforts, discuss common caveats, and provide suggestions for the development of novel techniques in this area.
        </td></tr>
    </tbody></table> -->

    

    <!-- <table id="Eric Xing">
        <tbody><tr>
            <td class="date" rowspan="3">
                2pm
            </td>
            <td class="title">
                <font color="red"> (Keynote) </font> Collaborative Learning in Medical Imaging
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.nmr.mgh.harvard.edu/user/8165/"><b>Jayashree Kalpathy-Cramer</b></a> (UC Boulder)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Machine learning has shown impressive potential in healthcare, particularly in medical imaging. A lack of access to care in many parts of the globe highlights the need to develop safe and equitable algorithms using diverse datasets.  Despite a surge in research applying deep learning (DL) to problems in healthcare, there remains a gap in its translational impact. Critical hurdles in safely deploying DL algorithms are concerns around brittleness, bias and fairness. The creation of extensive, multi-institutional datasets can enhance model performance and generalizability, but assembling such datasets is challenging due to patient privacy concerns, regulatory hurdles, and financial constraints. Collaborative learning offers a promising way to build more robust models by leveraging diverse datasets without the need to share the data directly. Foundational approaches have also been proposed to address some of these challenges. But challenges remain when dealing with small or heterogenous datasets, as is frequently seen in healthcare.This talk will explore collaborative learning applications in fields such as radiology, oncology, and ophthalmology. We will wrap up with an overview of the practical and theoretical challenges faced in implementing collaborative learning in healthcare contexts.
        </td></tr>
    </tbody></table>

    <table id="Eric Xing">
        <tbody><tr>
            <td class="date" rowspan="3">
                2:30pm
            </td>
            <td class="title">
                <font color="red"> (Keynote) </font> Humanitarian Collaborative Learning
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.anniehartley.info/"><b>Mary-Anne Hartley</b></a> (Yale and EPFL)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Humanitarian response aims to reduce preventable deaths and uphold human rights in situations of acute need at scale, such as wars, conflicts, and natural disasters. Organizations such as the International Committee of the Red Cross (ICRC) have been meticulously documenting their interventions in billions of pieces of multimodal data for over a century across 160 countries during countless emergencies. Their data represent the world’s most vulnerable populations and contain invaluable information that could better inform their own responses, as well as broader and longer-term initiatives to reduce inequities, such as the United Nations' Sustainable Development Goals.
 
In practice, however,the data is fragmented across multiple humanitarian actors who do not have the capacity to analyze, harmonize, or anonymize it, and ultimately, this precious information is rarely used beyond basic internal reporting in Excel sheets. 

In this talk, I will share my experience of collaborating with several NGOs to make data-driven tools for humanitarian interventions. I will make a semantic mapping of the seven cardinal humanitarian principles (neutrality, impartiality, independence, humanity, voluntary service, unity, universality) with distributed learning and also highlight methodological approaches that could integrate humanitarian principles into the design of these tools to better ensure real-world adoption.
        </td></tr>
    </tbody></table> -->



  <!--   <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                3:00pm
            </td>
            <td class="title-special">
                Panel Discussion and Coffee
            </td>
        </tr>
        <tr>
        </tr>
    </tbody></table> -->


    

    

    


</body></html>